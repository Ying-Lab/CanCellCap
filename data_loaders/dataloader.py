import pandas as pd 
import torch
import os
import glob
from torch.utils.data import TensorDataset,DataLoader
from TISCH import *
import anndata
import scanpy as sc
import sys

def train_dataloaders(args,shuffle_state=True):
    """
    Create training dataloaders

    Args:
        args (object) : args object generated by opt_util.py
        shuffle_state (bool) : Whether to shuffle the data

    Returns:
        a list of dataloader
    """

    batch_size = args.batch_size
    # TRAIN_GENE_LIST = os.path.join(args.output,args.genelist_outname)

    need_col_num=args.gene_num
    label_str = args.label_str
    # gene_list = pd.read_csv(TRAIN_GENE_LIST,header=None,index_col=0)
    gene_list = pd.DataFrame(index=args.HVG_list)
    gene_list.index = gene_list.index.append(pd.Index(label_str))
    train_loader_list = []
    
    data_spl_files = args.data_spl_files
    print(f"train dataset: {data_spl_files}")

    for data_spl_file in sorted(data_spl_files):
        if data_spl_file.endswith('.parquet'):
            raw_df = pd.read_parquet(data_spl_file)
        elif data_spl_file.endswith('.csv'):
            raw_df = pd.read_csv(data_spl_file,index_col=0)
        else:
            continue

        raw_df = pd.merge(gene_list,raw_df,how='left',left_index=True,right_index=True).fillna(0)
        raw_df = raw_df[~raw_df.index.duplicated()]
        raw_df = raw_df.loc[gene_list.index]
        raw_df = raw_df.T

        print(f"dataset: {data_spl_file}, shape: {raw_df.shape}")

        train_df = raw_df
        print(f"train dataset: {train_df.shape}")

        # bulid train dataloder  
        X = train_df.iloc[:,:need_col_num].to_numpy()
        Y = train_df.loc[:,'label'].to_numpy()

        Y_tissue = train_df.loc[:,'tissue_label'].to_numpy()
        Y_cancer_type = train_df.loc[:,'cancer_label'].to_numpy()

        raw_set = TensorDataset(torch.from_numpy(X).float(),torch.from_numpy(Y).float(),torch.from_numpy(Y_tissue).float(),torch.from_numpy(Y_cancer_type).float())
        train_set = raw_set
        train_loader = DataLoader(dataset=train_set,batch_size = batch_size,shuffle=shuffle_state,drop_last=True)
        train_loader_list.append(train_loader)

    return train_loader_list


def val_dataloaders(args,shuffle_state=True):
    """
    Create validating dataloaders

    Args:
        args (object) : args object generated by opt_util.py
        shuffle_state (bool) : Whether to shuffle the data

    Returns:
        a dict of validating dataloaders
    """
    SPL_PATH = args.val_dir
    batch_size = args.batch_size

    need_col_num=args.gene_num
    label_str = args.label_str
    # TRAIN_GENE_LIST = os.path.join(args.output,args.genelist_outname)
    # gene_list = pd.read_csv(TRAIN_GENE_LIST,header=None,index_col=0)
    gene_list = pd.DataFrame(index=args.HVG_list)
    gene_list.index = gene_list.index.append(pd.Index(label_str))
    external_val_loader_dict = {}
    # data_spl_files = [os.path.join(SPL_PATH,"Bond.csv")]
    data_spl_files = []
    if data_spl_files==[]:
        data_spl_files = glob.glob(os.path.join(SPL_PATH,'*.parquet'))

    for data_spl_file in sorted(data_spl_files):
        if data_spl_file.endswith('.parquet'):
            raw_df = pd.read_parquet(data_spl_file)
        elif data_spl_file.endswith('.csv'):
            raw_df = pd.read_csv(data_spl_file,index_col=0)
        else:
            continue
        print(f"loading dataset: {data_spl_file}")
        raw_df = pd.merge(gene_list,raw_df,how='left',left_index=True,right_index=True).fillna(0)
        raw_df = raw_df[~raw_df.index.duplicated()]
        raw_df = raw_df.loc[gene_list.index]
        raw_df = raw_df.T

        print(f"success load datasize: {raw_df.shape}")
        # if NEED_ROWS > raw_df.shape[0] :
        #     raw_df = pd.DataFrame(np.repeat(raw_df.values,NEED_ROWS/(raw_df.shape[0]),axis=0),columns=raw_df.columns)
        # else :
        #     raw_df = raw_df.sample(NEED_ROWS,random_state=42)
        X = raw_df.iloc[:,:need_col_num].to_numpy()
        Y = raw_df.loc[:,'label'].to_numpy()
        Y_tissue = raw_df.loc[:,'tissue_label'].to_numpy()
        Y_cancer_type = raw_df.loc[:,'cancer_label'].to_numpy()

        raw_set = TensorDataset(torch.from_numpy(X).float(),torch.from_numpy(Y).float(),torch.from_numpy(Y_tissue).float(),torch.from_numpy(Y_cancer_type).float())
      
        val_set = raw_set
        val_loader = DataLoader(dataset=val_set,batch_size = batch_size,shuffle=shuffle_state,drop_last=False)
        external_val_loader_dict[os.path.basename(data_spl_file).split('.')[0]] = val_loader

    return external_val_loader_dict
    


def test_dataloader(args, val_dataset, log=True, shuffle_state=False):
    SPL_FILE = val_dataset
    batch_size = args.batch_size

    need_col_num=args.gene_num
    label_str = args.label_str

    gene_list = pd.DataFrame(index=args.HVG_list)
    gene_list.index = gene_list.index.append(pd.Index(label_str))
    external_val_loader_dict = {}

    if SPL_FILE.endswith('.parquet'):
        raw_df = pd.read_parquet(SPL_FILE) 
    elif SPL_FILE.endswith('.csv'):
        raw_df = pd.read_csv(SPL_FILE,index_col=0)

    if 'label' not in raw_df.columns:
        raw_df = raw_df.T
    
    if 'label' not in raw_df.columns:
        print("Error: 'label' column is missing. Exiting program.")
        sys.exit(1)  

    raw_df = raw_df.T
    raw_data = raw_df[:-1]
    Y = raw_df.loc['label'].copy().to_numpy()

    if log:
        adata = anndata.AnnData(X=raw_data.T)
        sc.pp.normalize_total(adata, target_sum=1e4)
        sc.pp.log1p(adata)
        log_df = pd.DataFrame(adata.X.T, index=raw_data.index, columns=raw_data.columns)
    else:
        log_df = raw_data

    merge_df = pd.merge(gene_list,log_df,how='left',left_index=True,right_index=True).fillna(0)
    merge_df = merge_df[~merge_df.index.duplicated()]
    merge_df = merge_df.loc[gene_list.index]
    merge_df = merge_df.T

    X = merge_df.iloc[:,:need_col_num].to_numpy()
    Y_tissue = merge_df.loc[:,'tissue_label'].copy().to_numpy()
    Y_cancer_type = merge_df.loc[:,'cancer_label'].copy().to_numpy()
    merge_df = TensorDataset(torch.from_numpy(X).float(),torch.from_numpy(Y).float(),torch.from_numpy(Y_tissue).float(),torch.from_numpy(Y_cancer_type).float())
    
    val_set = merge_df
    val_loader = DataLoader(dataset=val_set,batch_size = batch_size,shuffle=shuffle_state,drop_last=False)
    external_val_loader_dict[os.path.basename(SPL_FILE).split('.')[0]] = val_loader

    return external_val_loader_dict


